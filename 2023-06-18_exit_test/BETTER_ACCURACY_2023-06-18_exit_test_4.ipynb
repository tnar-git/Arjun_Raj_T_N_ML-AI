{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1BZtuS2NT2rEVQI3StN-ELxg6ynfTNC03","authorship_tag":"ABX9TyOZHhXoMCNM8/7PGZK0+T98"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"LSTfbEhBwuR-"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KEmoub7HNkev"},"outputs":[],"source":["import pandas as pd\n","import numpy as np"]},{"cell_type":"code","source":["# from google.colab import drive\n","# drive.mount('/content/drive')"],"metadata":{"id":"KnmXbP6twfL3"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yUmC2ue2N9zr"},"outputs":[],"source":["drive_project_folder_path='/content/drive/MyDrive/ml-ai files arjun'\n","datafile_path=f'{drive_project_folder_path}/tweet_emotions.csv'\n","# datafile_path=f'{drive_project_folder_path}/tweets.csv'\n","\n","\n","tweets_data = pd.read_csv(datafile_path)\n","old_tweets_data=pd.read_csv(f'{drive_project_folder_path}/tweets.csv')\n","\n","tweets_data.columns=['tweet_id','sentiment','content']"]},{"cell_type":"code","source":["tweets_data.info()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hOmHL9marYRR","executionInfo":{"status":"ok","timestamp":1687192120705,"user_tz":-330,"elapsed":281,"user":{"displayName":"Arjun Raj","userId":"12208118088799322719"}},"outputId":"75a72d6f-098e-4798-f84d-53471beda6ee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 40000 entries, 0 to 39999\n","Data columns (total 3 columns):\n"," #   Column     Non-Null Count  Dtype \n","---  ------     --------------  ----- \n"," 0   tweet_id   40000 non-null  int64 \n"," 1   sentiment  40000 non-null  object\n"," 2   content    40000 non-null  object\n","dtypes: int64(1), object(2)\n","memory usage: 937.6+ KB\n"]}]},{"cell_type":"code","source":["old_tweets_data.info()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g6GeNO9LrbYU","executionInfo":{"status":"ok","timestamp":1687192120707,"user_tz":-330,"elapsed":267,"user":{"displayName":"Arjun Raj","userId":"12208118088799322719"}},"outputId":"04bf7d54-c9ae-488d-aa6d-4a542af5580d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 7920 entries, 0 to 7919\n","Data columns (total 3 columns):\n"," #   Column  Non-Null Count  Dtype \n","---  ------  --------------  ----- \n"," 0   id      7920 non-null   int64 \n"," 1   label   7920 non-null   int64 \n"," 2   tweet   7920 non-null   object\n","dtypes: int64(2), object(1)\n","memory usage: 185.8+ KB\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_dmrpd7oOHuy","outputId":"870dca76-f36f-4f47-be9c-cb89c1809dae","executionInfo":{"status":"ok","timestamp":1687192120711,"user_tz":-330,"elapsed":255,"user":{"displayName":"Arjun Raj","userId":"12208118088799322719"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(40000, 3)"]},"metadata":{},"execution_count":304}],"source":["tweets_data.shape"]},{"cell_type":"code","source":["## remove trouble some rows\n","# tweets_data.loc[6340,:]['content'] ### the trouble causing row in tweets.csv\n","\n","# tweets_data.loc[[12867,   664,  1538,  3401,  2621,  9040, 11111,  9920,  8576,\n","#               852,  6806,  3181,  4865,  9257,  6342, 13986,  2657,  8390,\n","#              7244, 13810],:]\n"],"metadata":{"id":"powhcYSU_F_a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tweets_data.drop([12867,   664,  1538,  3401,  2621,  9040, 11111,  9920,  8576,\n","              852,  6806,  3181,  4865,  9257,  6342, 13986,  2657,  8390,\n","             7244, 13810],axis=0,inplace=True)"],"metadata":{"id":"t7lbRiMcCEM2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_targets=tweets_data['sentiment'].nunique()\n","num_targets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o1fTTQf8VbZP","executionInfo":{"status":"ok","timestamp":1687192120715,"user_tz":-330,"elapsed":237,"user":{"displayName":"Arjun Raj","userId":"12208118088799322719"}},"outputId":"e67c9dcc-f38a-4e47-e6d9-b4407877b2f3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["13"]},"metadata":{},"execution_count":307}]},{"cell_type":"code","source":["tweets_data['sentiment'].value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2Ksyv55lWwTf","executionInfo":{"status":"ok","timestamp":1687192120716,"user_tz":-330,"elapsed":222,"user":{"displayName":"Arjun Raj","userId":"12208118088799322719"}},"outputId":"b18e5f5e-461c-41f7-c3e6-9069290ff160"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["neutral       8626\n","worry         8458\n","happiness     5205\n","sadness       5165\n","love          3841\n","surprise      2186\n","fun           1776\n","relief        1526\n","hate          1323\n","empty          826\n","enthusiasm     759\n","boredom        179\n","anger          110\n","Name: sentiment, dtype: int64"]},"metadata":{},"execution_count":308}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YjMwfa7XpSTI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687192120718,"user_tz":-330,"elapsed":209,"user":{"displayName":"Arjun Raj","userId":"12208118088799322719"}},"outputId":"7791156a-30fc-41ba-886b-4536658123f0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(14999, 3)"]},"metadata":{},"execution_count":309}],"source":["### data too big system crashes\n","tweets_data=tweets_data.iloc[1:15000,:]\n","tweets_data.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x7FzXEhKOJ-6","outputId":"188ce44d-2488-4889-8267-63deb1e6ab47","executionInfo":{"status":"ok","timestamp":1687192120720,"user_tz":-330,"elapsed":195,"user":{"displayName":"Arjun Raj","userId":"12208118088799322719"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['tweet_id', 'sentiment', 'content'], dtype='object')"]},"metadata":{},"execution_count":310}],"source":["tweets_data.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":520},"id":"iIfGRl7PONIW","outputId":"7a035c11-6016-4b2f-dbbf-279576508b79","executionInfo":{"status":"ok","timestamp":1687192120721,"user_tz":-330,"elapsed":181,"user":{"displayName":"Arjun Raj","userId":"12208118088799322719"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["      tweet_id   sentiment                                            content\n","1   1956967666     sadness  Layin n bed with a headache  ughhhh...waitin o...\n","2   1956967696     sadness                Funeral ceremony...gloomy friday...\n","3   1956967789  enthusiasm               wants to hang out with friends SOON!\n","4   1956968416     neutral  @dannycastillo We want to trade with someone w...\n","5   1956968477       worry  Re-pinging @ghostridah14: why didn't you go to...\n","6   1956968487     sadness  I should be sleep, but im not! thinking about ...\n","7   1956968636       worry               Hmmm. http://www.djhero.com/ is down\n","8   1956969035     sadness            @charviray Charlene my love. I miss you\n","9   1956969172     sadness         @kelcouch I'm sorry  at least it's Friday?\n","10  1956969456     neutral                                   cant fall asleep\n","11  1956969531       worry                            Choked on her retainers\n","12  1956970047     sadness  Ugh! I have to beat this stupid song to get to...\n","13  1956970424     sadness  @BrodyJenner if u watch the hills in london u ...\n","14  1956970860    surprise                                       Got the news\n","15  1956971077     sadness      The storm is here and the electricity is gone"],"text/html":["\n","  <div id=\"df-2389b56f-4c0f-42b8-bc71-778bb7ceb2b1\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tweet_id</th>\n","      <th>sentiment</th>\n","      <th>content</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>1956967666</td>\n","      <td>sadness</td>\n","      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1956967696</td>\n","      <td>sadness</td>\n","      <td>Funeral ceremony...gloomy friday...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1956967789</td>\n","      <td>enthusiasm</td>\n","      <td>wants to hang out with friends SOON!</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1956968416</td>\n","      <td>neutral</td>\n","      <td>@dannycastillo We want to trade with someone w...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>1956968477</td>\n","      <td>worry</td>\n","      <td>Re-pinging @ghostridah14: why didn't you go to...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>1956968487</td>\n","      <td>sadness</td>\n","      <td>I should be sleep, but im not! thinking about ...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>1956968636</td>\n","      <td>worry</td>\n","      <td>Hmmm. http://www.djhero.com/ is down</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>1956969035</td>\n","      <td>sadness</td>\n","      <td>@charviray Charlene my love. I miss you</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>1956969172</td>\n","      <td>sadness</td>\n","      <td>@kelcouch I'm sorry  at least it's Friday?</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>1956969456</td>\n","      <td>neutral</td>\n","      <td>cant fall asleep</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>1956969531</td>\n","      <td>worry</td>\n","      <td>Choked on her retainers</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>1956970047</td>\n","      <td>sadness</td>\n","      <td>Ugh! I have to beat this stupid song to get to...</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>1956970424</td>\n","      <td>sadness</td>\n","      <td>@BrodyJenner if u watch the hills in london u ...</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>1956970860</td>\n","      <td>surprise</td>\n","      <td>Got the news</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>1956971077</td>\n","      <td>sadness</td>\n","      <td>The storm is here and the electricity is gone</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2389b56f-4c0f-42b8-bc71-778bb7ceb2b1')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-2389b56f-4c0f-42b8-bc71-778bb7ceb2b1 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-2389b56f-4c0f-42b8-bc71-778bb7ceb2b1');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":311}],"source":["tweets_data.head(15)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":38},"id":"ulW5nHEiOQNl","outputId":"f0148195-a7e9-4deb-dd6b-80a58113379d","executionInfo":{"status":"ok","timestamp":1687192120721,"user_tz":-330,"elapsed":113,"user":{"displayName":"Arjun Raj","userId":"12208118088799322719"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'@dannycastillo We want to trade with someone who has Houston tickets, but no one will.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":312}],"source":["tweets_data['content'][4]"]},{"cell_type":"code","source":["tweets_data.isna().sum()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hGud23QzQAwt","executionInfo":{"status":"ok","timestamp":1687192120723,"user_tz":-330,"elapsed":112,"user":{"displayName":"Arjun Raj","userId":"12208118088799322719"}},"outputId":"e592d197-cbce-4d9f-ce93-0dbfbb332f03"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tweet_id     0\n","sentiment    0\n","content      0\n","dtype: int64"]},"metadata":{},"execution_count":313}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xwqaU66_OUPG"},"outputs":[],"source":["# tweets_data['sentiment'][1]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W637aCZkObvI","outputId":"5204cabe-c88c-4835-b591-747d72dd4198","executionInfo":{"status":"ok","timestamp":1687192120725,"user_tz":-330,"elapsed":105,"user":{"displayName":"Arjun Raj","userId":"12208118088799322719"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["worry         4665\n","sadness       3333\n","neutral       2788\n","surprise       813\n","hate           786\n","happiness      684\n","love           583\n","relief         336\n","fun            335\n","empty          313\n","enthusiasm     196\n","boredom        103\n","anger           64\n","Name: sentiment, dtype: int64"]},"metadata":{},"execution_count":315}],"source":["tweets_data['sentiment'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4DzD0FDTLNpK"},"outputs":[],"source":["tweets_data=tweets_data.drop('tweet_id',axis=1)\n","X=tweets_data.drop('sentiment',axis=1)\n","y=tweets_data['sentiment']"]},{"cell_type":"code","source":["### encode y\n","from sklearn.preprocessing import LabelEncoder\n","oe=LabelEncoder()\n","\n","# y=y.values.reshape(-1,1)\n","y_np=oe.fit_transform(y)\n","y[:]=y_np\n","y"],"metadata":{"id":"swYV6TPjV-Mc","executionInfo":{"status":"ok","timestamp":1687192120727,"user_tz":-330,"elapsed":101,"user":{"displayName":"Arjun Raj","userId":"12208118088799322719"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"49cc6f77-9822-4437-c4a5-c7b906b351dd"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1        10\n","2        10\n","3         3\n","4         8\n","5        12\n","         ..\n","15015    12\n","15016    12\n","15017     2\n","15018    12\n","15019    12\n","Name: sentiment, Length: 14999, dtype: object"]},"metadata":{},"execution_count":317}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ei3JFdj5LNpL"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train,y_test = train_test_split(X,y,test_size=0.2, random_state=1)"]},{"cell_type":"code","source":["X_train.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_2peNG1O9-4z","executionInfo":{"status":"ok","timestamp":1687192120729,"user_tz":-330,"elapsed":96,"user":{"displayName":"Arjun Raj","userId":"12208118088799322719"}},"outputId":"df8f7db1-fb35-414e-f6d7-0277650d2278"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(11999, 1)"]},"metadata":{},"execution_count":319}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t811NvOorF2e","outputId":"864eeeb7-b3cc-415a-9476-ea475874a0fe","executionInfo":{"status":"ok","timestamp":1687192120729,"user_tz":-330,"elapsed":89,"user":{"displayName":"Arjun Raj","userId":"12208118088799322719"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":320}],"source":["import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bvgU_zqwLNpM"},"outputs":[],"source":["from bs4 import BeautifulSoup\n","import re\n","#Removing the html strips\n","def strip_html(text):\n","    soup = BeautifulSoup(text, \"html.parser\")\n","    return soup.get_text()\n","\n","#Removing the square brackets\n","def remove_between_square_brackets(text):\n","    return re.sub('\\[[^]]*\\]', '', text)\n","\n","#Removing the noisy text\n","def denoise_text(text):\n","    text = strip_html(text)\n","    text = remove_between_square_brackets(text)\n","    return text\n","\n","\n","#Define function for removing special characters\n","def remove_special_characters(text, remove_digits=True):\n","    pattern=r'[^a-zA-z0-9\\s]'\n","    text=re.sub(pattern,'',text)\n","    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n","    text = re.sub(r\"what's\", \"what is \", text)\n","    text = re.sub(r\"\\'s\", \" \", text)\n","    text = re.sub(r\"\\'ve\", \" have \", text)\n","    text = re.sub(r\"can't\", \"cannot \", text)\n","    text = re.sub(r\"n't\", \" not \", text)\n","    text = re.sub(r\"I'm\", \"i am \", text)\n","    text = re.sub(r\"\\'re\", \" are \", text)\n","    text = re.sub(r\"\\'d\", \" would \", text)\n","    text = re.sub(r\"\\'ll\", \" will \", text)\n","    text = re.sub(r\",\", \" \", text)\n","    text = re.sub(r\"\\.\", \" \", text)\n","    text = re.sub(r\"!\", \" ! \", text)\n","    text = re.sub(r\"\\^^\", \"\", text)\n","    text = re.sub(r\"\\/\", \" \", text)\n","    text = re.sub(r\"\\^\", \" ^ \", text)\n","    text = re.sub(r\"\\+\", \" + \", text)\n","    text = re.sub(r\"\\-\", \" - \", text)\n","    text = re.sub(r\"\\=\", \" = \", text)\n","    text = re.sub(r\"'\", \" \", text)\n","    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n","    text = re.sub(r\":\", \" : \", text)\n","    text = re.sub(r\" e g \", \" eg \", text)\n","    text = re.sub(r\" b g \", \" bg \", text)\n","    text = re.sub(r\" u s \", \" american \", text)\n","    text = re.sub(r\"\\0s\", \"0\", text)\n","    text = re.sub(r\" 9 11 \", \"911\", text)\n","    text = re.sub(r\"e - mail\", \"email\", text)\n","    text = re.sub(r\"j k\", \"jk\", text)\n","    text = re.sub(r\"\\s{2,}\", \" \", text)\n","    return text\n","\n","def lemmatize_text(text):\n","    lem=nltk.stem.wordnet.WordNetLemmatizer()\n","    text= ' '.join([lem.lemmatize(word) for word in text.split()])\n","    return text\n","\n","#removing the stopwords\n","from nltk.tokenize.toktok import ToktokTokenizer\n","def remove_stopwords(text, is_lower_case=False):\n","    tokenizer1=ToktokTokenizer()\n","    stopword_list=nltk.corpus.stopwords.words('english')\n","    tokens = tokenizer1.tokenize(text)\n","    tokens = [token.strip() for token in tokens]\n","    if is_lower_case:\n","        filtered_tokens = [token for token in tokens if token not in stopword_list]\n","    else:\n","        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n","    filtered_text = ' '.join(filtered_tokens)\n","    return filtered_text\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6-KBTC87OmV7"},"outputs":[],"source":["from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn.pipeline import FeatureUnion, Pipeline\n","from gensim.utils import simple_preprocess\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","from nltk.tokenize import word_tokenize\n","from sklearn.preprocessing import StandardScaler\n","\n","class DummyTransformer(BaseEstimator,TransformerMixin):\n","\n","    '''\n","    dummy class to inherit from to avoid typing the fit method for everything\n","    '''\n","    def __init__(self):\n","        return None\n","    def fit(self,X=None,y=None):\n","        return self\n","    def transform(self,X=None):\n","        return X\n","\n","def do_basic_text_preprocessing(text:str):\n","    preprocessed_text=denoise_text(text)\n","    preprocessed_text=remove_special_characters(preprocessed_text)\n","    preprocessed_text=remove_stopwords(preprocessed_text)\n","    preprocessed_text=lemmatize_text(preprocessed_text)\n","\n","\n","    return preprocessed_text\n","\n","class TextPreprocessor(DummyTransformer):\n","    def transform(self,X:pd.DataFrame):\n","#         preprocessed_X_np_array=X.apply(do_basic_text_preprocessing).values\n","        # preprocessed_X_nparr=X['content'].apply(lambda x:do_basic_text_preprocessing(x)).values\n","        preprocessed_X_df=X['content'].apply(lambda x:do_basic_text_preprocessing(x))\n","        return preprocessed_X_df\n","\n","class Tokenizer(DummyTransformer):\n","    def transform(self, X):\n","        X=pd.DataFrame(X)\n","        # print(X.info())\n","        # print(X)\n","        X_tokenized=X['content'].apply(lambda x:simple_preprocess(x))\n","        # X_tokenized=X.apply(word_tokenize,axis=0) ## requires downloading punkt\n","        # print(X_tokenized)\n","        return X_tokenized\n","\n","\n","def get_nan_when_regex_does_not_match(x,pattern='^\\s+$'):\n","    if re.search(pattern,x) == None:\n","        return np.nan\n","    else:\n","        return 1\n","\n","\n","\n","\n","from nltk.tokenize.toktok import ToktokTokenizer\n","from nltk.tokenize import word_tokenize\n","\n","class NonGensimTokenizer(DummyTransformer):\n","    def transform(self, X):\n","        X=pd.DataFrame(X)\n","        # print(X.info())\n","        # print(X)\n","        print('has NaN?   ',X.isna().sum())\n","        temp_df=X['content'].apply(lambda x:get_nan_when_regex_does_not_match(x,'^\\s*$'))\n","        print('has empty string ?  ',len(temp_df)-int(temp_df.isna().sum()))\n","        print(X.info())\n","        print('this is X[content] ',X['content'])\n","        # X_tokenized=X['content'].apply(lambda x:ToktokTokenizer.tokenize(x))\n","        X_tokenized=X['content'].apply(lambda x:word_tokenize(x))\n","        # X_tokenized=X.apply(word_tokenize,axis=0) ## requires downloading punkt\n","        # print(X_tokenized)\n","        return X_tokenized\n","\n","class SparseToDenseArr(DummyTransformer):\n","    def transform(self,X=None):\n","#         preprocessed_X_np_array=X.apply(do_basic_text_preprocessing).values\n","        dense_arr=X.toarray()\n","\n","        return dense_arr\n","\n","class MetaFeatureEngineer(DummyTransformer):\n","    def transform(self, X=None,y=None):\n","        return None\n","\n","tfidf_meta_union_inst=FeatureUnion([('tfidf',TfidfVectorizer()),('metafeature',MetaFeatureEngineer())])\n","\n","# tfidf_pipeline=Pipeline([('textpreprocessor',TextPreprocessor()),('main',tfidf_meta_union_inst),('sparsetodense',SparseToDenseArr())])\n","tfidf_pipeline=Pipeline([('textpreprocessor',TextPreprocessor()),('main',tfidf_meta_union_inst),('std_scaler',StandardScaler())])\n","\n","\n","# tfidf_pipeline=Pipeline([('textpreprocessor',TextPreprocessor()),('tfidf',TfidfVectorizer()),('sparsetodense',SparseToDenseArr())])\n","\n","\n","bow_meta_union_inst=FeatureUnion([('count',CountVectorizer()),('metafeature',MetaFeatureEngineer())])\n","\n","# bow_pipeline=Pipeline([('textpreprocessor',TextPreprocessor()),('main',bow_meta_union_inst),('sparsetodense',SparseToDenseArr())])\n","# bow_pipeline=Pipeline([('textpreprocessor',TextPreprocessor()),('main',bow_meta_union_inst),('sparsetodense',SparseToDenseArr()),('std_scaler',StandardScaler())])\n","bow_pipeline=Pipeline([('textpreprocessor',TextPreprocessor()),('count',CountVectorizer()),('sparsetodense',SparseToDenseArr()),('std_scaler',StandardScaler())])\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l5QeS7fn_z0N"},"outputs":[],"source":["### word2vec\n","from gensim.models import Word2Vec\n","from gensim.models import KeyedVectors\n","\n","class CustomWord2VecTransformer(DummyTransformer):\n","    def __init__(self,**kwargs):\n","        self.input_args=kwargs\n","        self.count=0\n","\n","    def fit(self,X,y=None):\n","        self.w2v_model=Word2Vec(X,**self.input_args)\n","\n","        return self\n","\n","\n","    def get_mean_embedding_for_doc(self,doc_tokens):\n","        self.count+=1\n","        model=self.w2v_model\n","        embeddings=[]\n","        for tok in doc_tokens:\n","            if tok in model.wv.index_to_key:\n","                embeddings.append(model.wv.get_vector(tok))\n","        mean_embedding=np.mean(embeddings,axis=0)\n","        # if self.count>2:\n","        #     pass\n","        # else:\n","        #     print()\n","        return mean_embedding\n","\n","    def transform(self, X): ## successfully troubleshooted; troubleshot transform function\n","        # print('before  ',X.shape)\n","\n","        X_transformed=X.apply(lambda x:self.get_mean_embedding_for_doc(x))\n","        # print('x_transformed before .values ',X_transformed)\n","        # print(X_transformed.shape)\n","        # print('x_transformed before .tolist ; info ',X_transformed.info())\n","\n","        # X_transformed=pd.DataFrame(X_transformed.tolist())\n","        self.nan_index=X_transformed.index[X_transformed.isna()]\n","        # print(self.nan_index)\n","        X_transformed=X_transformed.dropna()\n","        X_transformed=pd.DataFrame(X_transformed.values.tolist()) ## still float not iterable error\n","        # X_transformed=pd.DataFrame(list(X_transformed.values))\n","        # X_transformed=pd.DataFrame(X_transformed.values)\n","        # print('x_transformed after .tolist ',X_transformed)\n","        # # print('x_transformed after .tolist ; info ',np.info(X_transformed))\n","        # print('x_transformed after .tolist ; info ',X_transformed.info())\n","        # print('x_transformed after .tolist ; dtypes ',X_transformed.dtypes)\n","\n","        # print(X_transformed)\n","        return X_transformed\n","\n","    # def transform(self, X): ### original transform\n","    #     print('before  ',X.shape)\n","\n","    #     X_transformed=X.apply(lambda x:self.get_mean_embedding_for_doc(x))\n","    #     print('x_transformed before .values ',X_transformed)\n","    #     print(X_transformed.shape)\n","    #     X_transformed=pd.DataFrame(X_transformed.tolist())\n","    #     # X_transformed=pd.DataFrame(X_transformed.values)\n","    #     print('x_transformed after .values ',X_transformed)\n","    #     # print(X_transformed)\n","    #     return X_transformed\n","\n","\n","class GoogleWord2VecTransformer(DummyTransformer):\n","    def __init__(self,*args,**kwargs):\n","        self.w2vmodel=KeyedVectors.load_word2vec_format(*args,**kwargs)\n","\n","        self.input_args=kwargs\n","\n","\n","    def fit(self,X,y=None):\n","\n","        return self\n","\n","    def get_mean_embedding_for_doc(self,doc_tokens):\n","        model=self.w2vmodel\n","        embeddings=[]\n","        for tok in doc_tokens:\n","            if tok in model:\n","                embeddings.append(model[tok])\n","        return np.mean(embeddings,axis=0)\n","\n","\n","    # def transform(self, X): ### old\n","    #     X_transformed=X.apply(lambda x:self.get_mean_embedding_for_doc(x))\n","\n","\n","    #     X_transformed=pd.DataFrame(X_transformed.tolist())\n","    #     return X_transformed\n","\n","    def transform(self, X): ## successfully troubleshooted; troubleshot transform function\n","        # print('before  ',X.shape)\n","\n","        X_transformed=X.apply(lambda x:self.get_mean_embedding_for_doc(x))\n","        # print('x_transformed before .values ',X_transformed)\n","        # print(X_transformed.shape)\n","        # print('x_transformed before .tolist ; info ',X_transformed.info())\n","\n","        # X_transformed=pd.DataFrame(X_transformed.tolist())\n","        self.nan_index=X_transformed.index[X_transformed.isna()]\n","        # print(self.nan_index)\n","        X_transformed=X_transformed.dropna()\n","        X_transformed=pd.DataFrame(X_transformed.values.tolist()) ## still float not iterable error\n","        # X_transformed=pd.DataFrame(list(X_transformed.values))\n","        # X_transformed=pd.DataFrame(X_transformed.values)\n","        # print('x_transformed after .tolist ',X_transformed)\n","        # # print('x_transformed after .tolist ; info ',np.info(X_transformed))\n","        # print('x_transformed after .tolist ; info ',X_transformed.info())\n","        # print('x_transformed after .tolist ; dtypes ',X_transformed.dtypes)\n","        return X_transformed\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GUsKjP5M5mlm"},"outputs":[],"source":["'''\n","# google_meta_union_inst=FeatureUnion([\n","#     ('google',GoogleWord2VecTransformer(\"/content/drive/MyDrive/ml-ai files arjun/GoogleNews-vectors-negative300.bin\",binary=True)),\n","#     ('metafeature',MetaFeatureEngineer())])\n","'''\n","\n","# from google.colab import drive\n","# drive.mount('/content/drive')\n","# google_trans_inst=GoogleWord2VecTransformer(f\"{drive_project_folder_path}/GoogleNews-vectors-negative300.bin\",binary=True)\n"]},{"cell_type":"code","source":["# from google.colab import drive\n","# drive.mount('/content/drive')"],"metadata":{"id":"m_4Hv6hZ6T-r"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"53abysqYbRlT"},"outputs":[],"source":["google_pipeline=Pipeline([('textpreprocessor',TextPreprocessor()),('tokenizer',Tokenizer()),('main',google_trans_inst)])\n","cbow_custom_w2v_pipeline=Pipeline([('textpreprocessor',TextPreprocessor()),('tokenizer',Tokenizer()),('main',CustomWord2VecTransformer(vector_size = 300, window = 6 , min_count = 3, sg = 0))])\n","skgram_w2v_pipeline=cbow_custom_w2v_pipeline.set_params(main=CustomWord2VecTransformer(vector_size = 300, window = 6 , min_count = 3, sg = 1))\n","\n","# cbow_custom_w2v_pipeline=Pipeline([('textpreprocessor',TextPreprocessor()),('tokenizer',NonGensimTokenizer()),('main',CustomWord2VecTransformer(vector_size = 300, window = 6 , min_count = 3, sg = 0))])\n","# skgram_w2v_pipeline=cbow_custom_w2v_pipeline.set_params(main=CustomWord2VecTransformer(vector_size = 300, window = 6 , min_count = 3, sg = 1))\n","\n","\n","\n","\n"]},{"cell_type":"code","source":["# prepro_pipe=Pipeline([('textpreprocessor',TextPreprocessor()),('tokenizer',Tokenizer())])\n","\n","# X_trans=prepro_pipe.fit_transform(X_train)\n","# X_trans.shape\n"],"metadata":{"id":"bKVCmwW5_ZNi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# cus=CustomWord2VecTransformer(vector_size = 300, window = 6 , min_count = 3, sg = 0)\n","# x_cus=cus.fit_transform(X_trans)"],"metadata":{"id":"jHwHNF9nAXIF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.naive_bayes import GaussianNB\n","from xgboost import XGBClassifier\n","from sklearn.ensemble import RandomForestClassifier , AdaBoostClassifier\n","from sklearn.svm import SVC\n","\n","result_dict={}"],"metadata":{"id":"nHffagy4Dgwe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","# pipelines=[tfidf_pipeline,bow_pipeline,google_pipeline,cbow_custom_w2v_pipeline,skgram_w2v_pipeline]\n","# pipeline_names=['tfidf_pipeline','bow_pipeline','google_pipeline','cbow_custom_w2v_pipeline','skgram_w2v_pipeline']\n","\n","pipelines=[google_pipeline,skgram_w2v_pipeline,bow_pipeline] ## testing\n","pipeline_names=['google_pipeline','skgram_w2v_pipeline','bow_pipeline']\n","\n","\n","# pipelines=[cbow_custom_w2v_pipeline,bow_pipeline] ## testing\n","# pipeline_names=['cbow_custom_w2v_pipeline','bow_pipeline']\n","\n","# pipelines=[bow_pipeline] ## testing\n","# pipeline_names=['bow_pipeline']\n","\n","count=0\n","for pipe in pipelines:\n","    pipe_name=pipeline_names[count]\n","    y_train=pd.DataFrame(y_train).astype('category')\n","    y_test=pd.DataFrame(y_test).astype('category')\n","    X_train_transformed=pipe.fit_transform(X_train)\n","    dropped_nan_index_train=pipe.named_steps['main'].nan_index.tolist()\n","    # X_train_transformed=X_train_transformed.drop(dropped_nan_index_train,axis=0)\n","    y_train_nan_dropped=y_train.drop(dropped_nan_index_train,axis=0)\n","\n","    X_test_transformed=pipe.transform(X_test)\n","    dropped_nan_index_test=pipe.named_steps['main'].nan_index.tolist()\n","    y_test_nan_dropped=y_test.drop(dropped_nan_index_test,axis=0)\n","    X_train_to_use=X_train_transformed\n","    X_test_to_use=X_test_transformed\n","    y_train_to_use=y_train_nan_dropped\n","    y_test_to_use=y_test_nan_dropped\n","    print(pipe_name)\n","    # type(pipe[\"main\"]).__name__\n","    # algo_insts=[GaussianNB(),DecisionTreeClassifier(),LogisticRegression()] ##\n","    # algo_insts=[XGBClassifier(),RandomForestClassifier(),AdaBoostClassifier()] ##\n","    algo_insts=[RandomForestClassifier(),SVC(kernel='linear'),XGBClassifier()] ##\n","\n","\n","    count+=1\n","    for algo_inst in algo_insts:\n","        y_train_to_use=pd.DataFrame(y_train_to_use)\n","        y_train_to_use=y_train_to_use.values.flatten()\n","        algo_inst.fit(X_train_to_use, y_train_to_use)\n","        y_pred=algo_inst.predict(X_test_to_use)\n","        accuracy=accuracy_score(y_test_to_use,y_pred)\n","        result_dict.update({f'{pipe_name}_{type(algo_inst).__name__}':{'accuracy':accuracy}})\n","        print(f'{pipe_name}_{type(algo_inst).__name__}   ',accuracy)\n","pd.DataFrame(result_dict).transpose()\n","\n"],"metadata":{"id":"hrPDxFEPzl3p","outputId":"c080e5b3-ff05-4368-d782-48ec385204e9","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["<ipython-input-321-05424db898d3>:5: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n","  soup = BeautifulSoup(text, \"html.parser\")\n","/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n","  return _methods._mean(a, axis=axis, dtype=dtype,\n","<ipython-input-321-05424db898d3>:5: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n","  soup = BeautifulSoup(text, \"html.parser\")\n","/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n","  return _methods._mean(a, axis=axis, dtype=dtype,\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["google_pipeline\n","google_pipeline_RandomForestClassifier    0.3436241610738255\n","google_pipeline_SVC    0.34328859060402683\n","google_pipeline_XGBClassifier    0.34496644295302015\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["<ipython-input-321-05424db898d3>:5: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n","  soup = BeautifulSoup(text, \"html.parser\")\n","/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n","  return _methods._mean(a, axis=axis, dtype=dtype,\n","<ipython-input-321-05424db898d3>:5: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n","  soup = BeautifulSoup(text, \"html.parser\")\n","/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n","  return _methods._mean(a, axis=axis, dtype=dtype,\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["skgram_w2v_pipeline\n","skgram_w2v_pipeline_RandomForestClassifier    0.3119946091644205\n","skgram_w2v_pipeline_SVC    0.3244609164420485\n"]}]},{"cell_type":"code","source":["pipe.named_steps['main'].nan_index"],"metadata":{"id":"mn_4jOBr_slA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_pred"],"metadata":{"id":"JVxXZVYr_srA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(y_pred))\n","y_pred_df=pd.DataFrame(y_pred)\n","print(y_pred_df.isna().sum())\n","pd.DataFrame(y_pred).value_counts()\n"],"metadata":{"id":"8otPzYolQ-2w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(y_test_to_use))\n","y_test_to_use.value_counts()"],"metadata":{"id":"ujYiH02__sxS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(y_test_to_use.isna().sum())"],"metadata":{"id":"ON01wnxXS6zK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["accuracy_score(y_test_to_use.astype('category'),pd.DataFrame(y_pred).astype('category'))\n"],"metadata":{"id":"SuN_W6KoSDNz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nltk.download(\"punkt\")\n","from nltk.tokenize import word_tokenize\n","from nltk import FreqDist\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing import sequence\n","\n","text_col=X['content']\n","all_words = \" \".join(text_col)\n","all_words = word_tokenize(all_words)\n","dist = FreqDist(all_words)\n","del all_words\n","num_unique_word = len(dist)\n","\n","### get max words per document\n","r_len = []\n","\n","for text in X:\n","    word = word_tokenize(text)\n","    l = len(word)\n","    r_len.append(l)\n","\n","MAX_TWEET_LEN = np.max(r_len)\n","del r_len\n","class TensorflowTokenizer(DummyTransformer):\n","    def transform(self,X=None):\n","        # text_col=X['content']\n","        text_col=X\n","        tokenizer = Tokenizer(num_words = num_unique_word)\n","        tokenizer.fit_on_texts(list(text_col))\n","        text_col= tokenizer.texts_to_sequences(text_col)\n","        text_col = sequence.pad_sequences(text_col, MAX_TWEET_LEN)\n","        return text_col\n","\n","keras_pipe= Pipeline([('textpreprocessor',TextPreprocessor()),('tensorflowtokenizer',TensorflowTokenizer())])\n","\n","pipe = keras_pipe\n","X_train_transformed=pipe.fit_transform(X_train)\n","X_test_transformed=pipe.transform(X_test)\n","X_train_to_use=X_train_transformed\n","X_test_to_use=X_test_transformed\n","y_train_to_use=y_train\n","y_test_to_use=y_test\n","\n"],"metadata":{"id":"XtD9EZJqKJkD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Dropout, Embedding,Flatten"],"metadata":{"id":"Orn-V3kEQ5hB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Sequential()\n","model.add(Embedding(input_dim = num_unique_word, output_dim = 150, input_length = MAX_TWEET_LEN))\n","\n","model.add(LSTM(128, dropout = 0.2 ))\n","\n","model.add(Dense(num_targets, activation = \"softmax\"))\n","model.add(Flatten())"],"metadata":{"id":"o53ZZIOWQvVq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_train_to_use=y_train_to_use.astype(np.int).flatten()\n","\n","# y_train_to_use = np.asarray(y_train_to_use).astype('float32').reshape((-1,1))\n","# y_train_to_use"],"metadata":{"id":"hbaN-o8mZeuD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow import keras"],"metadata":{"id":"-SZceDx-0O5v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_train_to_use=keras.utils.to_categorical(y_train_to_use,num_targets)\n","model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = ['accuracy'])\n","history1 = model.fit(X_train_to_use, y_train_to_use, epochs = 60, batch_size = 32)"],"metadata":{"id":"kkyPNfz3RAlU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.metrics_names"],"metadata":{"id":"wqIH_RdGQi8g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_test_to_use=y_test_to_use.astype(np.int).flatten()\n","y_test_to_use=keras.utils.to_categorical(y_test_to_use,num_targets)\n","\n","eval_result=model.evaluate(X_test_to_use,y_test_to_use)\n","result_dict.update({f'keras':{'accuracy':eval_result}})\n","\n","pd.DataFrame(result_dict).transpose()['accuracy'].max()\n","\n","# pd.DataFrame(result_dict).transpose()"],"metadata":{"id":"pzIFMTc2z0I7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle\n","dbfile = open(f'{drive_project_folder_path}/pipelines.pkl', 'ab')\n","\n","pickle.dump(pipelines, dbfile)\n","dbfile.close()\n","\n","\n","dbfile = open(f'{drive_project_folder_path}/result_dict.pkl', 'ab')\n","\n","pickle.dump(result_dict, dbfile)\n","dbfile.close()\n","\n","\n","dbfile = open(f'{drive_project_folder_path}/history1.pkl', 'ab')\n","\n","pickle.dump(history1, dbfile)\n","dbfile.close()\n"],"metadata":{"id":"3X8ZvZ_7HN-z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history1 = model.fit(X_train_to_use, y_train_to_use, epochs = 10, batch_size = 32)"],"metadata":{"id":"IsKfXZzPTwEo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"G-Swhu8_VAsc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["eval_result=model.evaluate(X_test_to_use,y_test_to_use)\n","result_dict.update({f'keras':{'accuracy':eval_result}})\n","\n","pd.DataFrame(result_dict).transpose()['accuracy'].max()"],"metadata":{"id":"FlWHN8o8Ty-L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle\n","dbfile = open(f'{drive_project_folder_path}/pipelines.pkl', 'ab')\n","\n","pickle.dump(pipelines, dbfile)\n","dbfile.close()\n","\n","\n","dbfile = open(f'{drive_project_folder_path}/result_dict.pkl', 'ab')\n","\n","pickle.dump(result_dict, dbfile)\n","dbfile.close()\n","\n","\n","dbfile = open(f'{drive_project_folder_path}/history1.pkl', 'ab')\n","\n","pickle.dump(history1, dbfile)\n","dbfile.close()\n"],"metadata":{"id":"Zh4ey5_uT1AM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"7bWXVIJwH4gh"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"uoRTwMI4LNpP"},"outputs":[],"source":["\n","# X_train_tfidf=custom_w2v_pipeline.fit_transform(X_train)\n","# X_test_tfidf=custom_w2v_pipeline.transform(X_test)\n","\n","# X_train=TextPreprocessor().transform(X_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_2YZPpH5LNpR"},"outputs":[],"source":["# print(np.sum(X_train_tfidf >0))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NNfCepzfTT54"},"outputs":[],"source":["### pass parameters to Pipeline steps"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nC7OHsTbLNpR"},"outputs":[],"source":["\n","\n","\n","# algos=[('naive_bayes','GaussianNB'),('tree','DecisionTreeClassifier'),('linear_model','LogisticRegression')]\n","# algos=[('linear_model','LogisticRegression')] ### why did i even do this?\n","\n","\n","\n","\n","# for algo in algos:\n","#     ## apparently the methods of the module are attributes of the obj outputted by import_module\n","#     class_var=getattr(import_module(f'sklearn.{algo[0]}'),algo[1])\n","#     clf=class_var()\n","#     # clf=module.\n","#     clf.fit(X_train_to_use, y_train_to_use)\n","#     y_pred=clf.predict(X_test_to_use)\n","#     result_dict.update({f'{type(clf).__name__}':{'accuracy':accuracy_score(y_pred,y_test_to_use)}})\n","# pd.DataFrame(result_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JQk3x_TGOxLQ"},"outputs":[],"source":["#### preprocess the text to that is in the required format for word2vec\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wmFBQDV4PixS"},"outputs":[],"source":["# print(simple_preprocess(tweets_data['content'][1])[:30])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HmJ6b2GgnJ8F"},"outputs":[],"source":["### google accuracy 0.8118686868686869\n","### cbow accuracy 0.8377525252525253\n"]}]}
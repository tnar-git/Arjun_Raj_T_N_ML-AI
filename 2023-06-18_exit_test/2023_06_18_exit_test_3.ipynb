{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KEmoub7HNkev"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xeZXjVTj8br1",
    "outputId": "877e3ee7-e7f4-447c-bc77-c468dfecc330"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yUmC2ue2N9zr"
   },
   "outputs": [],
   "source": [
    "drive_project_folder_path='/content/drive/MyDrive/ml-ai files arjun'\n",
    "datafile_path=f'{drive_project_folder_path}/tweet_emotions.csv'\n",
    "\n",
    "tweets_data = pd.read_csv(datafile_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_dmrpd7oOHuy",
    "outputId": "ce37c79b-2a43-49eb-da7d-d2d2f0f21d01"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o1fTTQf8VbZP",
    "outputId": "5c18d207-6238-4378-908f-8cd04bab2fbd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_targets=tweets_data['sentiment'].nunique()\n",
    "num_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Ksyv55lWwTf",
    "outputId": "c258b216-9df8-49ae-84a2-534241445b91"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral       8638\n",
       "worry         8459\n",
       "happiness     5209\n",
       "sadness       5165\n",
       "love          3842\n",
       "surprise      2187\n",
       "fun           1776\n",
       "relief        1526\n",
       "hate          1323\n",
       "empty          827\n",
       "enthusiasm     759\n",
       "boredom        179\n",
       "anger          110\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YjMwfa7XpSTI",
    "outputId": "19d53e18-687b-418c-9c08-f97497e35625"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14999, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### data too big system crashes\n",
    "tweets_data=tweets_data.iloc[1:15000,:]\n",
    "tweets_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x7FzXEhKOJ-6",
    "outputId": "671b7b85-4495-49c3-b6df-0d057fa07421"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tweet_id', 'sentiment', 'content'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "id": "iIfGRl7PONIW",
    "outputId": "387da5c9-d2bd-4cc6-87ba-006f973b72d6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-20b95d56-9b77-4f75-995a-02336a167e40\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1956968477</td>\n",
       "      <td>worry</td>\n",
       "      <td>Re-pinging @ghostridah14: why didn't you go to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1956968487</td>\n",
       "      <td>sadness</td>\n",
       "      <td>I should be sleep, but im not! thinking about ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1956968636</td>\n",
       "      <td>worry</td>\n",
       "      <td>Hmmm. http://www.djhero.com/ is down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1956969035</td>\n",
       "      <td>sadness</td>\n",
       "      <td>@charviray Charlene my love. I miss you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1956969172</td>\n",
       "      <td>sadness</td>\n",
       "      <td>@kelcouch I'm sorry  at least it's Friday?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1956969456</td>\n",
       "      <td>neutral</td>\n",
       "      <td>cant fall asleep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1956969531</td>\n",
       "      <td>worry</td>\n",
       "      <td>Choked on her retainers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1956970047</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Ugh! I have to beat this stupid song to get to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1956970424</td>\n",
       "      <td>sadness</td>\n",
       "      <td>@BrodyJenner if u watch the hills in london u ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1956970860</td>\n",
       "      <td>surprise</td>\n",
       "      <td>Got the news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1956971077</td>\n",
       "      <td>sadness</td>\n",
       "      <td>The storm is here and the electricity is gone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-20b95d56-9b77-4f75-995a-02336a167e40')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-20b95d56-9b77-4f75-995a-02336a167e40 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-20b95d56-9b77-4f75-995a-02336a167e40');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "      tweet_id   sentiment                                            content\n",
       "1   1956967666     sadness  Layin n bed with a headache  ughhhh...waitin o...\n",
       "2   1956967696     sadness                Funeral ceremony...gloomy friday...\n",
       "3   1956967789  enthusiasm               wants to hang out with friends SOON!\n",
       "4   1956968416     neutral  @dannycastillo We want to trade with someone w...\n",
       "5   1956968477       worry  Re-pinging @ghostridah14: why didn't you go to...\n",
       "6   1956968487     sadness  I should be sleep, but im not! thinking about ...\n",
       "7   1956968636       worry               Hmmm. http://www.djhero.com/ is down\n",
       "8   1956969035     sadness            @charviray Charlene my love. I miss you\n",
       "9   1956969172     sadness         @kelcouch I'm sorry  at least it's Friday?\n",
       "10  1956969456     neutral                                   cant fall asleep\n",
       "11  1956969531       worry                            Choked on her retainers\n",
       "12  1956970047     sadness  Ugh! I have to beat this stupid song to get to...\n",
       "13  1956970424     sadness  @BrodyJenner if u watch the hills in london u ...\n",
       "14  1956970860    surprise                                       Got the news\n",
       "15  1956971077     sadness      The storm is here and the electricity is gone"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_data.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 38
    },
    "id": "ulW5nHEiOQNl",
    "outputId": "e91f1b67-0d25-4a8e-9798-bdca2ae0e17f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'@dannycastillo We want to trade with someone who has Houston tickets, but no one will.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_data['content'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hGud23QzQAwt",
    "outputId": "fe9bfc06-5b97-45b2-b810-49c2c2eb74b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id     0\n",
       "sentiment    0\n",
       "content      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "xwqaU66_OUPG"
   },
   "outputs": [],
   "source": [
    "# tweets_data['sentiment'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W637aCZkObvI",
    "outputId": "64cbf3e4-aca9-429b-c982-f356b7bda334"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "worry         4660\n",
       "sadness       3329\n",
       "neutral       2799\n",
       "surprise       812\n",
       "hate           783\n",
       "happiness      686\n",
       "love           584\n",
       "fun            335\n",
       "relief         335\n",
       "empty          313\n",
       "enthusiasm     196\n",
       "boredom        103\n",
       "anger           64\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "4DzD0FDTLNpK"
   },
   "outputs": [],
   "source": [
    "tweets_data=tweets_data.drop('tweet_id',axis=1)\n",
    "X=tweets_data.drop('sentiment',axis=1)\n",
    "y=tweets_data['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "swYV6TPjV-Mc"
   },
   "outputs": [],
   "source": [
    "### encode y\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "oe=OrdinalEncoder()\n",
    "y=y.values.reshape(-1,1)\n",
    "y=oe.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ei3JFdj5LNpL"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train,y_test = train_test_split(X,y,test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_2peNG1O9-4z",
    "outputId": "06ca673c-3447-47cb-9aca-c5f03b090223"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11999, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t811NvOorF2e",
    "outputId": "def57b8c-275c-438f-a4c3-a172e1d68b96"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "bvgU_zqwLNpM"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "#Removing the html strips\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "#Removing the square brackets\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "#Removing the noisy text\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "#Define function for removing special characters\n",
    "def remove_special_characters(text, remove_digits=True):\n",
    "    pattern=r'[^a-zA-z0-9\\s]'\n",
    "    text=re.sub(pattern,'',text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"I'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\^^\", \"\", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lem=nltk.stem.wordnet.WordNetLemmatizer()\n",
    "    text= ' '.join([lem.lemmatize(word) for word in text.split()])\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "6-KBTC87OmV7"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class DummyTransformer(BaseEstimator,TransformerMixin):\n",
    "\n",
    "    '''\n",
    "    dummy class to inherit from to avoid typing the fit method for everything\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        return None\n",
    "    def fit(self,X=None,y=None):\n",
    "        return self\n",
    "    def transform(self,X=None):\n",
    "        return X\n",
    "\n",
    "def do_basic_text_preprocessing(text:str):\n",
    "    preprocessed_text=denoise_text(text)\n",
    "    preprocessed_text=remove_special_characters(preprocessed_text)\n",
    "    preprocessed_text=lemmatize_text(preprocessed_text)\n",
    "\n",
    "    return preprocessed_text\n",
    "\n",
    "class TextPreprocessor(DummyTransformer):\n",
    "    def transform(self,X:pd.DataFrame):\n",
    "#         preprocessed_X_np_array=X.apply(do_basic_text_preprocessing).values\n",
    "        # preprocessed_X_nparr=X['content'].apply(lambda x:do_basic_text_preprocessing(x)).values\n",
    "        preprocessed_X_df=X['content'].apply(lambda x:do_basic_text_preprocessing(x))\n",
    "        return preprocessed_X_df\n",
    "\n",
    "class Tokenizer(DummyTransformer):\n",
    "    def transform(self, X):\n",
    "        X=pd.DataFrame(X)\n",
    "        # print(X.info())\n",
    "        # print(X)\n",
    "        X_tokenized=X['content'].apply(lambda x:simple_preprocess(x))\n",
    "        # X_tokenized=X.apply(word_tokenize,axis=0) ## requires downloading punkt\n",
    "        # print(X_tokenized)\n",
    "        return X_tokenized\n",
    "\n",
    "class SparseToDenseArr(DummyTransformer):\n",
    "    def transform(self,X=None):\n",
    "#         preprocessed_X_np_array=X.apply(do_basic_text_preprocessing).values\n",
    "        dense_arr=X.toarray()\n",
    "\n",
    "        return dense_arr\n",
    "\n",
    "class MetaFeatureEngineer(DummyTransformer):\n",
    "    def transform(self, X=None,y=None):\n",
    "        return None\n",
    "\n",
    "tfidf_meta_union_inst=FeatureUnion([('tfidf',TfidfVectorizer()),('metafeature',MetaFeatureEngineer())])\n",
    "\n",
    "# tfidf_pipeline=Pipeline([('textpreprocessor',TextPreprocessor()),('main',tfidf_meta_union_inst),('sparsetodense',SparseToDenseArr())])\n",
    "tfidf_pipeline=Pipeline([('textpreprocessor',TextPreprocessor()),('main',tfidf_meta_union_inst),('std_scaler',StandardScaler())])\n",
    "\n",
    "\n",
    "# tfidf_pipeline=Pipeline([('textpreprocessor',TextPreprocessor()),('tfidf',TfidfVectorizer()),('sparsetodense',SparseToDenseArr())])\n",
    "\n",
    "\n",
    "bow_meta_union_inst=FeatureUnion([('count',CountVectorizer()),('metafeature',MetaFeatureEngineer())])\n",
    "\n",
    "# bow_pipeline=Pipeline([('textpreprocessor',TextPreprocessor()),('main',bow_meta_union_inst),('sparsetodense',SparseToDenseArr())])\n",
    "# bow_pipeline=Pipeline([('textpreprocessor',TextPreprocessor()),('main',bow_meta_union_inst),('sparsetodense',SparseToDenseArr()),('std_scaler',StandardScaler())])\n",
    "bow_pipeline=Pipeline([('textpreprocessor',TextPreprocessor()),('count',CountVectorizer()),('sparsetodense',SparseToDenseArr()),('std_scaler',StandardScaler())])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "l5QeS7fn_z0N"
   },
   "outputs": [],
   "source": [
    "### word2vec\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "class CustomWord2VecTransformer(DummyTransformer):\n",
    "    def __init__(self,**kwargs):\n",
    "        self.input_args=kwargs\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        self.w2v_model=Word2Vec(X,**self.input_args)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def get_mean_embedding_for_doc(self,doc_tokens):\n",
    "        model=self.w2v_model\n",
    "        embeddings=[]\n",
    "        for tok in doc_tokens:\n",
    "            if tok in model.wv.index_to_key:\n",
    "                embeddings.append(model.wv.get_vector(tok))\n",
    "        return np.mean(embeddings,axis=0)\n",
    "\n",
    "    # def transform(self, X): ## troubleshot transform function\n",
    "    #     print('before  ',X.shape)\n",
    "\n",
    "    #     X_transformed=X.apply(lambda x:self.get_mean_embedding_for_doc(x))\n",
    "    #     print(X_transformed)\n",
    "    #     print(X_transformed.shape)\n",
    "    #     # X_transformed=pd.DataFrame(X_transformed.tolist())\n",
    "    #     # print(X_transformed)\n",
    "    #     return X_transformed\n",
    "\n",
    "    def transform(self, X): ### real transform\n",
    "        print('before  ',X.shape)\n",
    "\n",
    "        X_transformed=X.apply(lambda x:self.get_mean_embedding_for_doc(x))\n",
    "        print(X_transformed)\n",
    "        print(X_transformed.shape)\n",
    "        X_transformed=pd.DataFrame(X_transformed.tolist())\n",
    "        # print(X_transformed)\n",
    "        return X_transformed\n",
    "\n",
    "\n",
    "class GoogleWord2VecTransformer(DummyTransformer):\n",
    "    def __init__(self,*args,**kwargs):\n",
    "        self.w2vmodel=KeyedVectors.load_word2vec_format(*args,**kwargs)\n",
    "\n",
    "        self.input_args=kwargs\n",
    "\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "\n",
    "        return self\n",
    "\n",
    "    def get_mean_embedding_for_doc(self,doc_tokens):\n",
    "        model=self.w2vmodel\n",
    "        embeddings=[]\n",
    "        for tok in doc_tokens:\n",
    "            if tok in model:\n",
    "                embeddings.append(model[tok])\n",
    "        return np.mean(embeddings,axis=0)\n",
    "\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_transformed=X.apply(lambda x:self.get_mean_embedding_for_doc(x))\n",
    "\n",
    "\n",
    "        X_transformed=pd.DataFrame(X_transformed.tolist())\n",
    "        return X_transformed\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "id": "GUsKjP5M5mlm",
    "outputId": "89abe2a7-af41-4003-c642-c1de3f473bac"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\n# google_meta_union_inst=FeatureUnion([\\n#     (\\'google\\',GoogleWord2VecTransformer(\"/content/drive/MyDrive/ml-ai files arjun/GoogleNews-vectors-negative300.bin\",binary=True)),\\n#     (\\'metafeature\\',MetaFeatureEngineer())])\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# google_meta_union_inst=FeatureUnion([\n",
    "#     ('google',GoogleWord2VecTransformer(\"/content/drive/MyDrive/ml-ai files arjun/GoogleNews-vectors-negative300.bin\",binary=True)),\n",
    "#     ('metafeature',MetaFeatureEngineer())])\n",
    "'''\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# google_trans_inst=GoogleWord2VecTransformer(f\"{drive_project_folder_path}/GoogleNews-vectors-negative300.bin\",binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "m_4Hv6hZ6T-r"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "53abysqYbRlT"
   },
   "outputs": [],
   "source": [
    "# google_pipeline=Pipeline([('textpreprocessor',TextPreprocessor()),('tokenizer',Tokenizer()),('main',google_trans_inst)])\n",
    "cbow_custom_w2v_pipeline=Pipeline([('textpreprocessor',TextPreprocessor()),('tokenizer',Tokenizer()),('main',CustomWord2VecTransformer(vector_size = 300, window = 6 , min_count = 3, sg = 0))])\n",
    "skgram_w2v_pipeline=cbow_custom_w2v_pipeline.set_params(main=CustomWord2VecTransformer(vector_size = 300, window = 6 , min_count = 3, sg = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bKVCmwW5_ZNi",
    "outputId": "0e98b99c-6308-43a2-c36c-1ab4f642745d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-6a28970ad7e6>:5: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11999,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepro_pipe=Pipeline([('textpreprocessor',TextPreprocessor()),('tokenizer',Tokenizer())])\n",
    "\n",
    "X_trans=prepro_pipe.fit_transform(X_train)\n",
    "X_trans.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "jHwHNF9nAXIF"
   },
   "outputs": [],
   "source": [
    "# cus=CustomWord2VecTransformer(vector_size = 300, window = 6 , min_count = 3, sg = 0)\n",
    "# x_cus=cus.fit_transform(X_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "nHffagy4Dgwe"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "result_dict={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 327
    },
    "id": "hrPDxFEPzl3p",
    "outputId": "2f4ad079-dac0-46b6-c6b5-bd13d6f7c43e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-6a28970ad7e6>:5: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n",
      "<ipython-input-19-6a28970ad7e6>:5: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bow_pipeline\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-0501732a-a919-474e-bc0c-a5bb2b14ffdf\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bow_pipeline_LogisticRegression</th>\n",
       "      <td>0.255667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0501732a-a919-474e-bc0c-a5bb2b14ffdf')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-0501732a-a919-474e-bc0c-a5bb2b14ffdf button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-0501732a-a919-474e-bc0c-a5bb2b14ffdf');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                 accuracy\n",
       "bow_pipeline_LogisticRegression  0.255667"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# pipelines=[tfidf_pipeline,bow_pipeline,google_pipeline,cbow_custom_w2v_pipeline,skgram_w2v_pipeline]\n",
    "# pipeline_names=['tfidf_pipeline','bow_pipeline','google_pipeline','cbow_custom_w2v_pipeline','skgram_w2v_pipeline']\n",
    "\n",
    "pipelines=[bow_pipeline] ## testing\n",
    "pipeline_names=['bow_pipeline']\n",
    "\n",
    "\n",
    "count=0\n",
    "for pipe in pipelines:\n",
    "    pipe_name=pipeline_names[count]\n",
    "    X_train_transformed=pipe.fit_transform(X_train)\n",
    "    X_test_transformed=pipe.transform(X_test)\n",
    "    X_train_to_use=X_train_transformed\n",
    "    X_test_to_use=X_test_transformed\n",
    "    y_train_to_use=y_train\n",
    "    y_test_to_use=y_test\n",
    "    print(pipe_name)\n",
    "    # type(pipe[\"main\"]).__name__\n",
    "    # algo_insts=[GaussianNB(),DecisionTreeClassifier(),LogisticRegression()] ##\n",
    "    algo_insts=[LogisticRegression()] ## train many sklearn lgorithms\n",
    "\n",
    "    count+=1\n",
    "    for algo_inst in algo_insts:\n",
    "        y_train_to_use=y_train_to_use.flatten()\n",
    "        algo_inst.fit(X_train_to_use, y_train_to_use)\n",
    "        y_pred=algo_inst.predict(X_test_to_use)\n",
    "        result_dict.update({f'{pipe_name}_{type(algo_inst).__name__}':{'accuracy':accuracy_score(y_pred,y_test_to_use)}})\n",
    "pd.DataFrame(result_dict).transpose()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XtD9EZJqKJkD",
    "outputId": "3a6ec626-1ab3-44af-8f96-f00728138f55"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "<ipython-input-19-6a28970ad7e6>:5: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "text_col=X['content']\n",
    "all_words = \" \".join(text_col)\n",
    "all_words = word_tokenize(all_words)\n",
    "dist = FreqDist(all_words)\n",
    "del all_words\n",
    "num_unique_word = len(dist)\n",
    "\n",
    "### get max words per document\n",
    "r_len = []\n",
    "\n",
    "for text in X:\n",
    "    word = word_tokenize(text)\n",
    "    l = len(word)\n",
    "    r_len.append(l)\n",
    "\n",
    "MAX_TWEET_LEN = np.max(r_len)\n",
    "del r_len\n",
    "class TensorflowTokenizer(DummyTransformer):\n",
    "    def transform(self,X=None):\n",
    "        # text_col=X['content']\n",
    "        text_col=X\n",
    "        tokenizer = Tokenizer(num_words = num_unique_word)\n",
    "        tokenizer.fit_on_texts(list(text_col))\n",
    "        text_col= tokenizer.texts_to_sequences(text_col)\n",
    "        text_col = sequence.pad_sequences(text_col, MAX_TWEET_LEN)\n",
    "        return text_col\n",
    "\n",
    "keras_pipe= Pipeline([('textpreprocessor',TextPreprocessor()),('tensorflowtokenizer',TensorflowTokenizer())])\n",
    "\n",
    "pipe = keras_pipe\n",
    "X_train_transformed=pipe.fit_transform(X_train)\n",
    "X_test_transformed=pipe.transform(X_test)\n",
    "X_train_to_use=X_train_transformed\n",
    "X_test_to_use=X_test_transformed\n",
    "y_train_to_use=y_train\n",
    "y_test_to_use=y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "Orn-V3kEQ5hB"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Embedding,Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "o53ZZIOWQvVq"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = num_unique_word, output_dim = 150, input_length = MAX_TWEET_LEN))\n",
    "\n",
    "model.add(LSTM(128, dropout = 0.2 ))\n",
    "\n",
    "model.add(Dense(num_targets, activation = \"softmax\"))\n",
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hbaN-o8mZeuD",
    "outputId": "78734b40-f893-4aba-cf74-d85cb9e69406"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-32-f1a95322ff9d>:1: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y_train_to_use=y_train_to_use.astype(np.int).flatten()\n"
     ]
    }
   ],
   "source": [
    "y_train_to_use=y_train_to_use.astype(np.int).flatten()\n",
    "\n",
    "# y_train_to_use = np.asarray(y_train_to_use).astype('float32').reshape((-1,1))\n",
    "# y_train_to_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "-SZceDx-0O5v"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kkyPNfz3RAlU",
    "outputId": "1efd9f79-fe83-491a-daf6-8fe809f76cb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "375/375 [==============================] - 248s 640ms/step - loss: 1.9833 - accuracy: 0.2979\n",
      "Epoch 2/60\n",
      "204/375 [===============>..............] - ETA: 1:43 - loss: 1.9700 - accuracy: 0.2999"
     ]
    }
   ],
   "source": [
    "y_train_to_use=keras.utils.to_categorical(y_train_to_use,num_targets)\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = ['accuracy'])\n",
    "history1 = model.fit(X_train_to_use, y_train_to_use, epochs = 60, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wqIH_RdGQi8g"
   },
   "outputs": [],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pzIFMTc2z0I7"
   },
   "outputs": [],
   "source": [
    "y_test_to_use=y_test_to_use.astype(np.int).flatten()\n",
    "y_test_to_use=keras.utils.to_categorical(y_test_to_use,num_targets)\n",
    "\n",
    "eval_result=model.evaluate(X_test_to_use,y_test_to_use)\n",
    "result_dict.update({f'keras':{'accuracy':eval_result}})\n",
    "\n",
    "pd.DataFrame(result_dict).transpose()['accuracy'].max()\n",
    "\n",
    "# pd.DataFrame(result_dict).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3X8ZvZ_7HN-z"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "dbfile = open(f'{drive_project_folder_path}/pipelines.pkl', 'ab')\n",
    "\n",
    "pickle.dump(pipelines, dbfile)\n",
    "dbfile.close()\n",
    "\n",
    "\n",
    "dbfile = open(f'{drive_project_folder_path}/result_dict.pkl', 'ab')\n",
    "\n",
    "pickle.dump(result_dict, dbfile)\n",
    "dbfile.close()\n",
    "\n",
    "\n",
    "dbfile = open(f'{drive_project_folder_path}/history1.pkl', 'ab')\n",
    "\n",
    "pickle.dump(history1, dbfile)\n",
    "dbfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IsKfXZzPTwEo"
   },
   "outputs": [],
   "source": [
    "history1 = model.fit(X_train_to_use, y_train_to_use, epochs = 10, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G-Swhu8_VAsc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FlWHN8o8Ty-L"
   },
   "outputs": [],
   "source": [
    "eval_result=model.evaluate(X_test_to_use,y_test_to_use)\n",
    "result_dict.update({f'keras':{'accuracy':eval_result}})\n",
    "\n",
    "pd.DataFrame(result_dict).transpose()['accuracy'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zh4ey5_uT1AM"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "dbfile = open(f'{drive_project_folder_path}/pipelines.pkl', 'ab')\n",
    "\n",
    "pickle.dump(pipelines, dbfile)\n",
    "dbfile.close()\n",
    "\n",
    "\n",
    "dbfile = open(f'{drive_project_folder_path}/result_dict.pkl', 'ab')\n",
    "\n",
    "pickle.dump(result_dict, dbfile)\n",
    "dbfile.close()\n",
    "\n",
    "\n",
    "dbfile = open(f'{drive_project_folder_path}/history1.pkl', 'ab')\n",
    "\n",
    "pickle.dump(history1, dbfile)\n",
    "dbfile.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7bWXVIJwH4gh"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uoRTwMI4LNpP"
   },
   "outputs": [],
   "source": [
    "\n",
    "# X_train_tfidf=custom_w2v_pipeline.fit_transform(X_train)\n",
    "# X_test_tfidf=custom_w2v_pipeline.transform(X_test)\n",
    "\n",
    "# X_train=TextPreprocessor().transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_2YZPpH5LNpR"
   },
   "outputs": [],
   "source": [
    "# print(np.sum(X_train_tfidf >0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NNfCepzfTT54"
   },
   "outputs": [],
   "source": [
    "### pass parameters to Pipeline steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nC7OHsTbLNpR"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# algos=[('naive_bayes','GaussianNB'),('tree','DecisionTreeClassifier'),('linear_model','LogisticRegression')]\n",
    "# algos=[('linear_model','LogisticRegression')] ### why did i even do this?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for algo in algos:\n",
    "#     ## apparently the methods of the module are attributes of the obj outputted by import_module\n",
    "#     class_var=getattr(import_module(f'sklearn.{algo[0]}'),algo[1])\n",
    "#     clf=class_var()\n",
    "#     # clf=module.\n",
    "#     clf.fit(X_train_to_use, y_train_to_use)\n",
    "#     y_pred=clf.predict(X_test_to_use)\n",
    "#     result_dict.update({f'{type(clf).__name__}':{'accuracy':accuracy_score(y_pred,y_test_to_use)}})\n",
    "# pd.DataFrame(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JQk3x_TGOxLQ"
   },
   "outputs": [],
   "source": [
    "#### preprocess the text to that is in the required format for word2vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wmFBQDV4PixS"
   },
   "outputs": [],
   "source": [
    "# print(simple_preprocess(tweets_data['content'][1])[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HmJ6b2GgnJ8F"
   },
   "outputs": [],
   "source": [
    "### google accuracy 0.8118686868686869\n",
    "### cbow accuracy 0.8377525252525253\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

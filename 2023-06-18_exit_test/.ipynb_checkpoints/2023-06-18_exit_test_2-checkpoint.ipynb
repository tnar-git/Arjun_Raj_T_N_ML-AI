{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KEmoub7HNkev"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "yUmC2ue2N9zr"
   },
   "outputs": [],
   "source": [
    "tweets_data = pd.read_csv('tweet_emotions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_dmrpd7oOHuy",
    "outputId": "95862610-bea6-4a5e-804b-fa3e9e6aaac2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "YjMwfa7XpSTI"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x7FzXEhKOJ-6",
    "outputId": "c8337edc-7d65-46a0-a714-10d3237398fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tweet_id', 'sentiment', 'content'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "id": "iIfGRl7PONIW",
    "outputId": "fb4e3386-d7f0-4b98-a303-0402c21ada87"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1956968477</td>\n",
       "      <td>worry</td>\n",
       "      <td>Re-pinging @ghostridah14: why didn't you go to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1956968487</td>\n",
       "      <td>sadness</td>\n",
       "      <td>I should be sleep, but im not! thinking about ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1956968636</td>\n",
       "      <td>worry</td>\n",
       "      <td>Hmmm. http://www.djhero.com/ is down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1956969035</td>\n",
       "      <td>sadness</td>\n",
       "      <td>@charviray Charlene my love. I miss you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1956969172</td>\n",
       "      <td>sadness</td>\n",
       "      <td>@kelcouch I'm sorry  at least it's Friday?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1956969456</td>\n",
       "      <td>neutral</td>\n",
       "      <td>cant fall asleep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1956969531</td>\n",
       "      <td>worry</td>\n",
       "      <td>Choked on her retainers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1956970047</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Ugh! I have to beat this stupid song to get to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1956970424</td>\n",
       "      <td>sadness</td>\n",
       "      <td>@BrodyJenner if u watch the hills in london u ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1956970860</td>\n",
       "      <td>surprise</td>\n",
       "      <td>Got the news</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      tweet_id   sentiment                                            content\n",
       "0   1956967341       empty  @tiffanylue i know  i was listenin to bad habi...\n",
       "1   1956967666     sadness  Layin n bed with a headache  ughhhh...waitin o...\n",
       "2   1956967696     sadness                Funeral ceremony...gloomy friday...\n",
       "3   1956967789  enthusiasm               wants to hang out with friends SOON!\n",
       "4   1956968416     neutral  @dannycastillo We want to trade with someone w...\n",
       "5   1956968477       worry  Re-pinging @ghostridah14: why didn't you go to...\n",
       "6   1956968487     sadness  I should be sleep, but im not! thinking about ...\n",
       "7   1956968636       worry               Hmmm. http://www.djhero.com/ is down\n",
       "8   1956969035     sadness            @charviray Charlene my love. I miss you\n",
       "9   1956969172     sadness         @kelcouch I'm sorry  at least it's Friday?\n",
       "10  1956969456     neutral                                   cant fall asleep\n",
       "11  1956969531       worry                            Choked on her retainers\n",
       "12  1956970047     sadness  Ugh! I have to beat this stupid song to get to...\n",
       "13  1956970424     sadness  @BrodyJenner if u watch the hills in london u ...\n",
       "14  1956970860    surprise                                       Got the news"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_data.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "ulW5nHEiOQNl",
    "outputId": "c0401056-77bf-4daf-93a1-8ee4a41037c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What amazing service! Apple won't even talk to me about a question I have unless I pay them $19.95 for their stupid support!\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_data['content'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "xwqaU66_OUPG"
   },
   "outputs": [],
   "source": [
    "# tweets_data['sentiment'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W637aCZkObvI",
    "outputId": "baa3a7b1-c2e4-494f-d3b0-1d3a1ad6bb14"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5894\n",
       "1    2026\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "4DzD0FDTLNpK"
   },
   "outputs": [],
   "source": [
    "# tweets_data=tweets_data.drop('tweet_id',axis=1)\n",
    "X=tweets_data.drop('sentiment',axis=1)\n",
    "y=tweets_data['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "ei3JFdj5LNpL"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train,y_test = train_test_split(X,y,test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t811NvOorF2e",
    "outputId": "c75a1f34-461d-4562-bcfd-d54b4cd4bed0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/arjun/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/arjun/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/arjun/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/arjun/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "bvgU_zqwLNpM"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "#Removing the html strips\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "#Removing the square brackets\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "#Removing the noisy text\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "#Define function for removing special characters\n",
    "def remove_special_characters(text, remove_digits=True):\n",
    "    pattern=r'[^a-zA-z0-9\\s]'\n",
    "    text=re.sub(pattern,'',text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"I'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\^^\", \"\", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lem=nltk.stem.wordnet.WordNetLemmatizer()\n",
    "    text= ' '.join([lem.lemmatize(word) for word in text.split()])\n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "6-KBTC87OmV7"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "class DummyTransformer(BaseEstimator,TransformerMixin):\n",
    "    '''\n",
    "    dummy class to inherit from to avoid typing the fit method for everything\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        return None\n",
    "    def fit(self,X=None,y=None):\n",
    "        return self\n",
    "    def transform(self,X=None):\n",
    "        return X\n",
    "    \n",
    "def do_basic_text_preprocessing(text:str):\n",
    "    preprocessed_text=denoise_text(text)\n",
    "    preprocessed_text=remove_special_characters(preprocessed_text)\n",
    "#     preprocessed_text=lemmatize_text(preprocessed_text)\n",
    "\n",
    "    return preprocessed_text\n",
    "\n",
    "class TextPreprocessor(DummyTransformer):\n",
    "    def transform(self,X:pd.DataFrame):\n",
    "        print(X.shape)\n",
    "        X['tweet']=X['tweet'].apply(do_basic_text_preprocessing)\n",
    "        preprocessed_X_df=X\n",
    "        return preprocessed_X_df\n",
    "\n",
    "# class Tokenizer(DummyTransformer):\n",
    "#     def transform(self, X):\n",
    "#         X=pd.DataFrame(X)\n",
    "#         # print(X.info())\n",
    "#         # print(X)\n",
    "#         X_tokenized=X['tweet'].apply(lambda x:simple_preprocess(x))\n",
    "#         # X_tokenized=X.apply(word_tokenize,axis=0) ## requires downloading punkt\n",
    "#         # print(X_tokenized)\n",
    "#         return X_tokenized\n",
    "    \n",
    "    \n",
    "class Tokenizer(DummyTransformer):\n",
    "    def transform(self, X):\n",
    "        print(X.shape)\n",
    "        X=pd.DataFrame(X)\n",
    "        X['tweet']=X['tweet'].apply(simple_preprocess)\n",
    "        X_tokenized=X['tweet']\n",
    "        return X_tokenized\n",
    "\n",
    "class SparseToDenseArr(DummyTransformer):\n",
    "    def transform(self,X=None):\n",
    "        dense_arr=X.toarray()\n",
    "        return dense_arr    \n",
    "\n",
    "class MetaFeatureEngineer(DummyTransformer):\n",
    "    def transform(self, X=None,y=None):\n",
    "        return None\n",
    "\n",
    "tfidf_meta_union_inst=FeatureUnion([('tfidf',TfidfVectorizer()),('metafeature',MetaFeatureEngineer())])\n",
    "    \n",
    "tfidf_pipeline=Pipeline([('textpreprocessor',TextPreprocessor()),('main',tfidf_meta_union_inst),('sparsetodense',SparseToDenseArr())])\n",
    "\n",
    "# tfidf_pipeline=Pipeline([('textpreprocessor',TextPreprocessor()),('tfidf',TfidfVectorizer()),('sparsetodense',SparseToDenseArr())])\n",
    "\n",
    "\n",
    "bow_meta_union_inst=FeatureUnion([('count',CountVectorizer()),('metafeature',MetaFeatureEngineer())])\n",
    "    \n",
    "bow_pipeline=Pipeline([('textpreprocessor',TextPreprocessor()),('main',bow_meta_union_inst),('sparsetodense',SparseToDenseArr())])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;main&#x27;,\n",
       "                 FeatureUnion(transformer_list=[(&#x27;tfidf&#x27;, TfidfVectorizer()),\n",
       "                                                (&#x27;metafeature&#x27;,\n",
       "                                                 MetaFeatureEngineer())])),\n",
       "                (&#x27;sparsetodense&#x27;, SparseToDenseArr())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;main&#x27;,\n",
       "                 FeatureUnion(transformer_list=[(&#x27;tfidf&#x27;, TfidfVectorizer()),\n",
       "                                                (&#x27;metafeature&#x27;,\n",
       "                                                 MetaFeatureEngineer())])),\n",
       "                (&#x27;sparsetodense&#x27;, SparseToDenseArr())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">main: FeatureUnion</label><div class=\"sk-toggleable__content\"><pre>FeatureUnion(transformer_list=[(&#x27;tfidf&#x27;, TfidfVectorizer()),\n",
       "                               (&#x27;metafeature&#x27;, MetaFeatureEngineer())])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>tfidf</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>metafeature</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MetaFeatureEngineer</label><div class=\"sk-toggleable__content\"><pre>MetaFeatureEngineer()</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SparseToDenseArr</label><div class=\"sk-toggleable__content\"><pre>SparseToDenseArr()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('main',\n",
       "                 FeatureUnion(transformer_list=[('tfidf', TfidfVectorizer()),\n",
       "                                                ('metafeature',\n",
       "                                                 MetaFeatureEngineer())])),\n",
       "                ('sparsetodense', SparseToDenseArr())])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### testing [[2023-05-11]]-0916-18\n",
    "tfidf_pipeline[1:]\n",
    "\n",
    "\n",
    "# class GenPreprocessor(DummyTransformer):\n",
    "#     def __init__(self,cols_list):\n",
    "#         self.cols_list=cols_list\n",
    "        \n",
    "#     def transform(self,X):\n",
    "#         X_to_process=X.loc[:,cols_list]\n",
    "#         X.loc[:,cols_list]=OrdinalEncoder.fit_transform(X_to_process)\n",
    "#         return X\n",
    "\n",
    "\n",
    "###  can't i just use a big columntransformer?\n",
    "## what about sparse matrix onehotencoder - will it output sparse matrix within pandas pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "l5QeS7fn_z0N"
   },
   "outputs": [],
   "source": [
    "### word2vec\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "class CustomWord2VecTransformer(DummyTransformer):\n",
    "    def __init__(self,**kwargs):\n",
    "        self.input_args=kwargs\n",
    "#         allowed_keys=['vector_size','window','min_count','sg']\n",
    "#         self.__dict__.update((k, v) for k, v in kwargs.items() if k in allowed_keys)\n",
    "    def fit(self,X,y=None):\n",
    "        self.w2v_model=Word2Vec(X,**self.input_args)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def get_mean_embedding_for_doc(self,doc_tokens):\n",
    "        model=self.w2v_model   \n",
    "        embeddings=[]\n",
    "        for tok in doc_tokens:\n",
    "            if tok in model.wv.index_to_key: ## to check if the the tok/word exists in the \n",
    "                                            ##  list of words converted to vectors by the model\n",
    "                embeddings.append(model.wv.get_vector(tok)) ## to get the vector(aka embedding) corresponding to the word\n",
    "        return np.mean(embeddings,axis=0)\n",
    "\n",
    "    \n",
    "    def transform(self, X):\n",
    "\n",
    "        X_transformed=X.apply(self.get_mean_embedding_for_doc)\n",
    "        X_transformed=pd.DataFrame(X_transformed.tolist())\n",
    "        # print(X_transformed)\n",
    "        return X_transformed\n",
    "\n",
    "\n",
    "class GoogleWord2VecTransformer(DummyTransformer):\n",
    "    def __init__(self,*args,**kwargs):\n",
    "#         GoogleWord2VecTransformer(\"/content/drive/MyDrive/ml-ai files arjun/GoogleNews-vectors-negative300.bin\",binary=True)\n",
    "        self.w2vmodel=KeyedVectors.load_word2vec_format(*args,**kwargs)\n",
    "\n",
    "        self.input_args=kwargs\n",
    "        \n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def get_mean_embedding_for_doc(self,doc_tokens):\n",
    "        model=self.w2vmodel\n",
    "        embeddings=[]\n",
    "        ## note the synatx to get vector etc. are different from that for gensim model\n",
    "        for tok in doc_tokens: \n",
    "            if tok in model: ## to check if the the tok/word exists in the \n",
    "                            ##  list of words converted to vectors by the model\n",
    "                embeddings.append(model[tok]) ## to get the vector(aka embedding) corresponding to the word\n",
    "        return np.mean(embeddings,axis=0)\n",
    "\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_transformed=X.apply(get_mean_embedding_for_doc)\n",
    "        X_transformed=pd.DataFrame(X_transformed.tolist())\n",
    "\n",
    "        return X_transformed\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "GUsKjP5M5mlm"
   },
   "outputs": [],
   "source": [
    "\n",
    "# google_meta_union_inst=FeatureUnion([\n",
    "#     ('google',GoogleWord2VecTransformer(\"/content/drive/MyDrive/ml-ai files arjun/GoogleNews-vectors-negative300.bin\",binary=True)),\n",
    "#     ('metafeature',MetaFeatureEngineer())])\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# google_trans_inst=GoogleWord2VecTransformer(\"/content/drive/MyDrive/ml-ai files arjun/GoogleNews-vectors-negative300.bin\",binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "53abysqYbRlT"
   },
   "outputs": [],
   "source": [
    "# google_pipeline=Pipeline([('textpreprocessor',TextPreprocessor()),('tokenizer',Tokenizer()),('main',google_trans_inst)])\n",
    "cbow_custom_w2v_pipeline=Pipeline([('textpreprocessor',TextPreprocessor()),('tokenizer',Tokenizer()),('main',CustomWord2VecTransformer(vector_size = 300, window = 6 , min_count = 3, sg = 0))])\n",
    "skgram_w2v_pipeline=cbow_custom_w2v_pipeline.set_params(main=CustomWord2VecTransformer(vector_size = 300, window = 6 , min_count = 3, sg = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'break_ties': False,\n",
       " 'cache_size': 200,\n",
       " 'class_weight': None,\n",
       " 'coef0': 0.0,\n",
       " 'decision_function_shape': 'ovr',\n",
       " 'degree': 3,\n",
       " 'gamma': 'scale',\n",
       " 'kernel': 'rbf',\n",
       " 'max_iter': -1,\n",
       " 'probability': False,\n",
       " 'random_state': None,\n",
       " 'shrinking': True,\n",
       " 'tol': 0.001,\n",
       " 'verbose': False}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "SVC().get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 891
    },
    "id": "hrPDxFEPzl3p",
    "outputId": "0c262f3b-96b1-48bf-c48f-ee37b49ec14d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6336, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arjun/anaconda3/lib/python3.9/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6336, 2)\n",
      "(1584, 2)\n",
      "(1584, 2)\n",
      "cbow_custom_w2v_pipeline\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cbow_custom_w2v_pipeline_LogisticRegression</th>\n",
       "      <td>0.874369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             accuracy\n",
       "cbow_custom_w2v_pipeline_LogisticRegression  0.874369"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from importlib import import_module\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "\n",
    "result_dict={}\n",
    "\n",
    "# pipelines=[tfidf_pipeline,bow_pipeline,google_pipeline,cbow_custom_w2v_pipeline,skgram_w2v_pipeline]\n",
    "# pipeline_names=['tfidf_pipeline','bow_pipeline','google_pipeline','cbow_custom_w2v_pipeline','skgram_w2v_pipeline']\n",
    "\n",
    "\n",
    "pipelines=[cbow_custom_w2v_pipeline]\n",
    "pipeline_names=['cbow_custom_w2v_pipeline']\n",
    "\n",
    "\n",
    "count=0\n",
    "for pipe in pipelines:\n",
    "    pipe_name=pipeline_names[count]\n",
    "    X_train_transformed=pipe.fit_transform(X_train)\n",
    "    X_test_transformed=pipe.transform(X_test)\n",
    "    \n",
    "    X_train_to_use=X_train_transformed\n",
    "    X_test_to_use=X_test_transformed\n",
    "    y_train_to_use=y_train\n",
    "    y_test_to_use=y_test\n",
    "    print(pipe_name)\n",
    "    # type(pipe[\"main\"]).__name__\n",
    "#     algo_insts=[GaussianNB(),DecisionTreeClassifier(),LogisticRegression()] ## \n",
    "    \n",
    "    algo_insts=[LogisticRegression(),SVC(kernel='poly',degree=2)] ## \n",
    "\n",
    "    count+=1\n",
    "    for algo_inst in algo_insts:\n",
    "        algo_inst.fit(X_train_to_use, y_train_to_use)\n",
    "        y_pred=algo_inst.predict(X_test_to_use)\n",
    "        result_dict.update({f'{pipe_name}_{type(algo_inst).__name__}':{'accuracy':accuracy_score(y_pred,y_test_to_use)}})\n",
    "pd.DataFrame(result_dict).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "pzIFMTc2z0I7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8743686868686869"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(result_dict).transpose()['accuracy'].max()\n",
    "\n",
    "\n",
    "# pd.DataFrame(result_dict).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "uoRTwMI4LNpP"
   },
   "outputs": [],
   "source": [
    "\n",
    "# X_train_tfidf=custom_w2v_pipeline.fit_transform(X_train)\n",
    "# X_test_tfidf=custom_w2v_pipeline.transform(X_test)\n",
    "\n",
    "# X_train=TextPreprocessor().transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "_2YZPpH5LNpR"
   },
   "outputs": [],
   "source": [
    "# print(np.sum(X_train_tfidf >0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "NNfCepzfTT54"
   },
   "outputs": [],
   "source": [
    "### pass parameters to Pipeline steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "nC7OHsTbLNpR"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# algos=[('naive_bayes','GaussianNB'),('tree','DecisionTreeClassifier'),('linear_model','LogisticRegression')]\n",
    "# algos=[('linear_model','LogisticRegression')] ### why did i even do this?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for algo in algos:\n",
    "#     ## apparently the methods of the module are attributes of the obj outputted by import_module\n",
    "#     class_var=getattr(import_module(f'sklearn.{algo[0]}'),algo[1]) \n",
    "#     clf=class_var()\n",
    "#     # clf=module.\n",
    "#     clf.fit(X_train_to_use, y_train_to_use)\n",
    "#     y_pred=clf.predict(X_test_to_use)\n",
    "#     result_dict.update({f'{type(clf).__name__}':{'accuracy':accuracy_score(y_pred,y_test_to_use)}})\n",
    "# pd.DataFrame(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "JQk3x_TGOxLQ"
   },
   "outputs": [],
   "source": [
    "#### preprocess the text to that is in the required format for word2vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "wmFBQDV4PixS"
   },
   "outputs": [],
   "source": [
    "# print(simple_preprocess(tweets_data['tweet'][1])[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "HmJ6b2GgnJ8F"
   },
   "outputs": [],
   "source": [
    "### google accuracy 0.8118686868686869\n",
    "### cbow accuracy 0.8377525252525253\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
